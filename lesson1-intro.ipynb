{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-Pytorch\" data-toc-modified-id=\"Intro-to-Pytorch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro to Pytorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pytorch-tensors\" data-toc-modified-id=\"Pytorch-tensors-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Pytorch tensors</a></span></li><li><span><a href=\"#Pytorch-Autograd\" data-toc-modified-id=\"Pytorch-Autograd-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pytorch Autograd</a></span></li><li><span><a href=\"#torch.nn-module\" data-toc-modified-id=\"torch.nn-module-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>torch.nn module</a></span></li></ul></li><li><span><a href=\"#Linear-Regression-with-Pytorch\" data-toc-modified-id=\"Linear-Regression-with-Pytorch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linear Regression with Pytorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-with-Pytorch\" data-toc-modified-id=\"Gradient-Descent-with-Pytorch-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Gradient Descent with Pytorch</a></span></li><li><span><a href=\"#Simplified-GD-Loop\" data-toc-modified-id=\"Simplified-GD-Loop-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Simplified GD Loop</a></span><ul class=\"toc-item\"><li><span><a href=\"#Models-in-Pytorch\" data-toc-modified-id=\"Models-in-Pytorch-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Models in Pytorch</a></span></li></ul></li></ul></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Dataset-and-Data-loaders\" data-toc-modified-id=\"Dataset-and-Data-loaders-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset and Data loaders</a></span></li><li><span><a href=\"#Two-layer-neural-network\" data-toc-modified-id=\"Two-layer-neural-network-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Two layer neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lab\" data-toc-modified-id=\"Lab-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Lab</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch consists of 4 main packages:\n",
    "* torch: a general purpose array library similar to Numpy that can do computations on GPU\n",
    "* torch.autograd: a package for automatically obtaining gradients - automatic diffrentiation of gradients\n",
    "* torch.nn: a neural net library with common layers and cost functions\n",
    "* torch.optim: an optimization package with common optimization algorithms like SGD, Adam, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors\n",
    "Like Numpy tensors but can utilize GPUs to accelerate its numerical computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random tensor\n",
    "N = 5\n",
    "x = torch.randn(N, 10).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn - Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttom = torch.tensor([[1,1,1,1],[2,2,2,2],[3,3,3,3]], dtype=torch.float32)\n",
    "ttom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttom.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ttom.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6372, -0.6402, -0.8900, -1.5058,  1.0412,  0.8828, -0.4196,  0.1894,\n",
       "          0.5029,  1.7505],\n",
       "        [-0.0274,  0.8677, -0.7222, -0.1081,  0.3971, -0.4973,  0.9475,  1.9302,\n",
       "         -0.2126, -0.2647],\n",
       "        [ 0.1088, -0.6865, -0.9363,  0.4614, -0.5689,  0.1683, -1.0017, -1.7360,\n",
       "         -0.4173,  0.6917],\n",
       "        [ 0.4672,  1.9870, -0.4882, -1.0205,  1.0594, -2.4485, -0.3497,  0.9914,\n",
       "         -1.1928,  1.0826],\n",
       "        [ 0.5940,  1.0454,  1.1251,  0.7670,  0.1333, -0.7561,  1.0782, -0.5954,\n",
       "          1.0970,  0.2733]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=5000)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6372, -0.6402, -0.8900, -1.5058,  1.0412,  0.8828, -0.4196,  0.1894,\n",
       "          0.5029,  1.7505, -0.0274,  0.8677, -0.7222, -0.1081,  0.3971, -0.4973,\n",
       "          0.9475,  1.9302, -0.2126, -0.2647,  0.1088, -0.6865, -0.9363,  0.4614,\n",
       "         -0.5689,  0.1683, -1.0017, -1.7360, -0.4173,  0.6917,  0.4672,  1.9870,\n",
       "         -0.4882, -1.0205,  1.0594, -2.4485, -0.3497,  0.9914, -1.1928,  1.0826,\n",
       "          0.5940,  1.0454,  1.1251,  0.7670,  0.1333, -0.7561,  1.0782, -0.5954,\n",
       "          1.0970,  0.2733]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping of tensors using .view()\n",
    "x.view(1,-1) #-1 makes torch infer the second dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "The autograd package in PyTorch provides classes and functions implementing automatic differentiation of arbitrary scalar valued function. For example, the gradient of the error with respect to all parameters.\n",
    "\n",
    "In order for this to happen we need to declare our paramerers as Tensors with the requires_grad=True keyword. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3., 4., 5., 6., 7., 8., 9., 10.], requires_grad=True) # These are parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(780., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = (2*x**2+1).sum()\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward() # computes the grad of L with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "\\newline\n",
       "\\dfrac{dy}{dx}(2x+1) = 2\n",
       "\\newline\n",
       "\\newline\n",
       "\\dfrac{dy}{dx}({2x^2+1}) = 4x\n",
       "\\newline\n",
       "\\newline\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "\\newline\n",
    "\\dfrac{dy}{dx}(2x+1) = 2\n",
    "\\newline\n",
    "\\newline\n",
    "\\dfrac{dy}{dx}({2x^2+1}) = 4x\n",
    "\\newline\n",
    "\\newline\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12., 16., 20., 24., 28., 32., 36., 40.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn module\n",
    "A neural net library with common layers and cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear(5, 3)` creates a linear transformation ($A\\cdot X+b$) of a $N \\times 5$ matrix into a $N \\times 3$ matrix, where N can be anything (number of observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```class torch.nn.Linear(in_features, out_features, bias=True)```\n",
    "\n",
    "Applies a linear transformation to the incoming data: y=x$A^T$+b\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html#linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5 # number of input featutes\n",
    "M = 3 # neurons in the first hidden layer\n",
    "linear_map = nn.Linear(D, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1016, -0.0086, -0.3081,  0.2965,  0.0314],\n",
       "         [ 0.4244,  0.3235, -0.2885,  0.3556,  0.1866],\n",
       "         [ 0.1347,  0.1490,  0.0316,  0.1586, -0.1341]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2590, -0.0960,  0.0310], requires_grad=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters are initialized randomly\n",
    "[p for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 5]), torch.Size([3])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to fit a line to a set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate some fake data\n",
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0,1,n) \n",
    "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
    "    return x, y\n",
    "\n",
    "x, y = gen_fake_data(500, 3., 8.) # a = 3 b = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to find **parameters** (weights) $a$ and $b$ such that you minimize the *error* between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error** ($\\sum_i (\\hat{y}_i - y_i)^2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat, y): return ((y_hat - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we believe $a = 10$ and $b = 5$ then we can compute `y_hat` which is our *prediction* and then compute our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.293934830787886"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lin(10,5,x)\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(a, b, x, y): return mse(lin(a,b,x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.293934830787886"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(10, 5, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for $a$ and $b$? How do we find the best *fitting* linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed dataset $x$ and $y$ `mse_loss(a,b)` is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function.\n",
    "\n",
    "**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.\n",
    "\n",
    "Here is gradient descent implemented in [PyTorch](http://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000,))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate some more data\n",
    "x, y = gen_fake_data(10000, 3., 8.)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap x and y as tensor \n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4657], dtype=torch.float64, requires_grad=True),\n",
       " tensor([0.4990], dtype=torch.float64, requires_grad=True))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random Tensors for weights, and wrap them in tensors.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "a, b = np.random.randn(1), np.random.randn(1)\n",
    "a = torch.tensor(a, requires_grad=True)\n",
    "b = torch.tensor(b, requires_grad=True)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08886463429957282\n",
      "0.08886463429957285\n",
      "0.08886463429957282\n",
      "0.0888646342995728\n",
      "0.08886463429957281\n",
      "0.08886463429957284\n",
      "0.08886463429957285\n",
      "0.08886463429957282\n",
      "0.08886463429957285\n",
      "0.08886463429957285\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    loss = mse_loss(a,b,x,y)\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "    \n",
    "    # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call a.grad and b.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to a and b respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update a and b using gradient descent; a.data and b.data are Tensors,\n",
    "    # a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    \n",
    "    # Zero the gradients\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9905], dtype=torch.float64, requires_grad=True) tensor([7.9965], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified GD Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear tranformation with input dimension=1 and output dimension=1\n",
    "nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple way of specifying a linear regression model\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(1, 1),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent way of specifiying the same model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        return x \n",
    "model =  LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note here we have just two parameters, why?\n",
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_fake_data(10000, 3., 8.)\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to be careful with the dimensions that your model is expecting\n",
    "x1 = torch.unsqueeze(x, 1)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x1)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    y_hat = model(x1)\n",
    "    loss = F.mse_loss(y_hat, y.unsqueeze(1))\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating fake data\n",
    "# Here we generate some fake data\n",
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_logistic_fake_data(n, a, b):\n",
    "    x = np.random.uniform(-20,20, (n, 2))\n",
    "    x2_hat = lin(a,b, x[:,0])\n",
    "    y = x[:,1] > x2_hat\n",
    "    return x, y.astype(int)\n",
    "\n",
    "x, y = gen_logistic_fake_data(100, 1., 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(-20, 20, 0.2)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[:,0],x[:,1],c=y, s=8);\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\");\n",
    "plt.plot(t, t + 0.5, 'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_logistic_fake_data(10000, 1., 0.5)\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    y_hat = model(x)\n",
    "    loss = F.binary_cross_entropy(F.sigmoid(y_hat), y.unsqueeze(1))\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Data loaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent** (GD). In GD you have to run through *all* the samples in your training set to do a single itaration. In SGD you use *only one* or *a subset*  of training samples to do the update for a parameter in a particular iteration. The subset use in every iteration is called a **batch** or **minibatch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0,1,n) \n",
    "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
    "    return x.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "# create a dataset\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, a=3, b=8, n=10000):\n",
    "        x, y = gen_fake_data(n, a, b)\n",
    "        x = torch.from_numpy(x).unsqueeze(1)\n",
    "        y = torch.from_numpy(y)\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "fake_dataset = RegressionDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create a data loader. The data loader provides the following features:\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(fake_dataset, batch_size=1000, shuffle=True)\n",
    "x, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(200):\n",
    "    for i, (x, y) in enumerate(dataloader): \n",
    "        \n",
    "        y_hat = model2(x)\n",
    "        loss = F.mse_loss(y_hat, y.unsqueeze(1))\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    if t % 50 == 0: print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([p for p in model2.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating fake data\n",
    "# Here we generate some fake data\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def gen_nn_fake_data(n):\n",
    "    x = np.random.uniform(0,10, (n, 2))\n",
    "    x1 = x[:,0]\n",
    "    x2 = x[:,1]\n",
    "    score1 = sigmoid(-x1 - 8* x2 + 50)\n",
    "    score2 = sigmoid(-7*x1 - 2* x2 + 50)\n",
    "    score3 = 2* score1 + 3*score2 - 0.1\n",
    "    y = score3 < 0\n",
    "    return x, y.astype(int)\n",
    "\n",
    "x, y = gen_nn_fake_data(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[:,0],x[:,1],c=y, s=8);\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 2),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(2, 1)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_nn_fake_data(10000)\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    y_hat = model(x)\n",
    "    loss = F.binary_cross_entropy(F.sigmoid(y_hat), y.unsqueeze(1))\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab\n",
    "* Get an AWS account and ask for GPU access.\n",
    "* Add a dataset and data loader to the neural network training.\n",
    "* Modify the neural network example to solve a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "* https://pytorch.org/docs/stable/index.html\n",
    "* http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "* https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logistic",
   "language": "python",
   "name": "logistic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "116px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
